{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb77060d-8df1-4d40-b286-fcca9be17f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "800329f6-9fb4-443e-9bd8-9d1bd38a3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e99bc-8e03-4d84-95b0-e8e608ac10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import get_openai_api_key\n",
    "\n",
    "openai_api_key = get_openai_api_key()\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-4.0-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd7108-b1aa-4a53-8119-05335476838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai_tools import ScrapeWebsiteTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a102f-7fe9-448e-bec7-6f65cfe7faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tool with the website URL, so the agent can only scrap the content of the specified website\n",
    "docs_scrape_tool = ScrapeWebsiteTool(\n",
    "    url=input(\"Enter website URL: \"), #\"https://www.aicamp.ai/event/eventdetails/W2025010100\"\n",
    "    website_url = url \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ee941-7483-4ee8-975f-678e4387e9c5",
   "metadata": {},
   "source": [
    "## Creating and run the Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a3e45-3539-42d0-a479-74dd4b14c060",
   "metadata": {},
   "source": [
    "Create your crew of Agents\n",
    "Pass the tasks to be performed by those agents.\n",
    "Note: For this simple example, the tasks will be performed sequentially (i.e they are dependent on each other), so the order of the task in the list matters.\n",
    "verbose=2 allows you to see all the logs of the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3498dd-88cf-487b-9943-514b2933262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Creat Agent\n",
    "    event_agent = Agent(\n",
    "        role=\"Event Content Researcher\",\n",
    "        goal=\"verify whether {text} is about an event information \"\n",
    "            \"collect the {text} as event_content and event url if it is event information\"\n",
    "            \"store the event data in JSON format\", \n",
    "        backstory=(\n",
    "            \"You need to verify the input information about events and\"\n",
    "            \"Check that the URL of the event is valid.\"\n",
    "            \"Make sure to provide event data in the required format, \"\n",
    "            \" and make no assumptions.\"\n",
    "        ),\n",
    "        allow_delegation=False,\n",
    "        verbose=True\n",
    "    )    \n",
    "\n",
    "\n",
    "    #creat task\n",
    "    webscraping_agent = Task(\n",
    "        description=(\n",
    "            \"If the {text} is not about an event information, ask user to input the event URL again.\"\n",
    "            \"If the {text} is about an event information,\"\n",
    "\t\t\t\"You need to scrap the event content on the web via the webpage per event URL.\"\n",
    "            \"Be sure to provide the event data including title, date, organization, leader or speaker or instructor, URL, and content.\"\n",
    "            \"You must store the event data in the required format\"\n",
    "            \"and accurate response to the user without assumption.\"\n",
    "        ),\n",
    "        expected_output=(\n",
    "            \"The event content from the webpage should be provided.\"\n",
    "            \"The event data is in JSON format.\"\n",
    "        ),\n",
    "        tools=[docs_scrape_tool],\n",
    "        agent=event_agent\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #creat the crew\n",
    "    crew = Crew(\n",
    "        agents=[event_agent],\n",
    "        tasks=[webscraping_agent]   \n",
    "    )\n",
    "    \n",
    "    result = crew.kickoff()\n",
    "    #print(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5256f4f-fae2-4d4e-a69b-10795f9e052c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
